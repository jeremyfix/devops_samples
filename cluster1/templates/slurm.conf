# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=master
#ControlAddr=
# 
MailProg=/usr/sbin/sendmail
MpiDefault=none
#MpiParams=ports=#-# 
ProctrackType=proctrack/pgid
ReturnToService=1
SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid
#SlurmctldPort=6817 
SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid
#SlurmdPort=6818 
SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd
SlurmUser=slurm
#SlurmdUser=root 
StateSaveLocation=/var/lib/slurm-llnl/slurmctld
SwitchType=switch/none
TaskPlugin=task/affinity
TaskPluginParam=Sched
# 
# 
# TIMERS 
#KillWait=30 
#MinJobAge=300 
#SlurmctldTimeout=120 
#SlurmdTimeout=300 
# 
# 
# SCHEDULING 
FastSchedule=1
SchedulerType=sched/backfill
#SchedulerPort=7321 
# cons_res: schedule individual cores
SelectType=select/cons_res
SelectTypeParameters=CR_Core
# this ensures submissions fail if they ask for more resources than available on the partition 
EnforcePartLimits=ALL
#
# 
# LOGGING AND ACCOUNTING 
AccountingStorageType=accounting_storage/none
ClusterName=cluster
#JobAcctGatherFrequency=30 
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=3 
SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log
#SlurmdDebug=3 
SlurmdLogFile=/var/log/slurm-llnl/slurmd.log
# 
# 
# BEGIN ANSIBLE MANAGED NODES
NodeName=node1 Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 State=UNKNOWN
NodeName=node2 Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 State=UNKNOWN
NodeName=node3 Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 State=UNKNOWN
NodeName=node4 Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 State=UNKNOWN
PartitionName=jobs Nodes=node[1-4] Default=YES MaxTime=INFINITE State=UP MaxNodes=1 MaxCPUsPerNode=1
# END ANSIBLE MANAGED NODES
